{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e6f702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3fe499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af425f2baba411e9182a1c5cc634b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2678bd54e8da440d81c7be2bab9aeb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# 1. 加载预训练模型和分词器\n",
    "model_name = \"gpt2\"  # 使用较小的模型作为演示\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. 加载基础模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3. 配置LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                      # LoRA的秩\n",
    "    lora_alpha=32,            # LoRA的alpha参数\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # 要应用LoRA的模块名称\n",
    "    lora_dropout=0.1,         # Dropout概率\n",
    "    bias=\"none\",              # 是否对偏置进行微调\n",
    "    task_type=TaskType.CAUSAL_LM  # 任务类型\n",
    ")\n",
    "\n",
    "# 4. 准备模型进行LoRA微调\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # 打印可训练参数比例\n",
    "\n",
    "# 5. 加载数据集（这里使用tiny_shakespeare作为示例）\n",
    "dataset = load_dataset(\"tiny_shakespeare\", trust_remote_code=True)\n",
    "# 获取完整文本\n",
    "full_text = dataset[\"train\"][\"text\"][0]  # 假设文本在\"text\"字段中\n",
    "\n",
    "# 手动划分\n",
    "split_point = int(len(full_text) * 0.9)  # 90%作为训练集\n",
    "train_text = full_text[:split_point]\n",
    "test_text = full_text[split_point:]\n",
    "\n",
    "# 创建新的数据集\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict({\"text\": [train_text]})\n",
    "test_dataset = Dataset.from_dict({\"text\": [test_text]})\n",
    "\n",
    "# 6. 数据预处理函数\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([text for text in examples[\"text\"]], truncation=True, max_length=128)\n",
    "\n",
    "# 7. 应用预处理\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")\n",
    "\n",
    "# 8. 数据整理器\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c9ca450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\envs\\LLM\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 13:53, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, we all know it.\n",
      "\n",
      "We know it.\n",
      "\n",
      "We know it.\n",
      "\n",
      "We know it.\n",
      "\n",
      "We know it.\n",
      "We know it.\n",
      "We know it.\n",
      "\n",
      "We know\n"
     ]
    }
   ],
   "source": [
    "# 9. 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned_gpt2\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 10. 初始化Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 11. 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 12. 保存模型\n",
    "model.save_pretrained(\"./lora_finetuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"./lora_finetuned_gpt2\")\n",
    "\n",
    "# 13. 测试微调后的模型\n",
    "prompt = \"To be or not to be,\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33589b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
