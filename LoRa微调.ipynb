{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e6f702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è®¾å¤‡: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3fe499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af425f2baba411e9182a1c5cc634b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2678bd54e8da440d81c7be2bab9aeb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "model_name = \"gpt2\"  # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä½œä¸ºæ¼”ç¤º\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3. é…ç½®LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                      # LoRAçš„ç§©\n",
    "    lora_alpha=32,            # LoRAçš„alphaå‚æ•°\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # è¦åº”ç”¨LoRAçš„æ¨¡å—åç§°\n",
    "    lora_dropout=0.1,         # Dropoutæ¦‚ç‡\n",
    "    bias=\"none\",              # æ˜¯å¦å¯¹åç½®è¿›è¡Œå¾®è°ƒ\n",
    "    task_type=TaskType.CAUSAL_LM  # ä»»åŠ¡ç±»å‹\n",
    ")\n",
    "\n",
    "# 4. å‡†å¤‡æ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒ\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹\n",
    "\n",
    "# 5. åŠ è½½æ•°æ®é›†ï¼ˆè¿™é‡Œä½¿ç”¨tiny_shakespeareä½œä¸ºç¤ºä¾‹ï¼‰\n",
    "dataset = load_dataset(\"tiny_shakespeare\", trust_remote_code=True)\n",
    "# è·å–å®Œæ•´æ–‡æœ¬\n",
    "full_text = dataset[\"train\"][\"text\"][0]  # å‡è®¾æ–‡æœ¬åœ¨\"text\"å­—æ®µä¸­\n",
    "\n",
    "# æ‰‹åŠ¨åˆ’åˆ†\n",
    "split_point = int(len(full_text) * 0.9)  # 90%ä½œä¸ºè®­ç»ƒé›†\n",
    "train_text = full_text[:split_point]\n",
    "test_text = full_text[split_point:]\n",
    "\n",
    "# åˆ›å»ºæ–°çš„æ•°æ®é›†\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict({\"text\": [train_text]})\n",
    "test_dataset = Dataset.from_dict({\"text\": [test_text]})\n",
    "\n",
    "# 6. æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([text for text in examples[\"text\"]], truncation=True, max_length=128)\n",
    "\n",
    "# 7. åº”ç”¨é¢„å¤„ç†\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")\n",
    "\n",
    "# 8. æ•°æ®æ•´ç†å™¨\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c9ca450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\envs\\LLM\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 13:53, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, we all know it.\n",
      "\n",
      "We know it.\n",
      "\n",
      "We know it.\n",
      "\n",
      "We know it.\n",
      "\n",
      "We know it.\n",
      "We know it.\n",
      "We know it.\n",
      "\n",
      "We know\n"
     ]
    }
   ],
   "source": [
    "# 9. è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned_gpt2\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 10. åˆå§‹åŒ–Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 11. å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "# 12. ä¿å­˜æ¨¡å‹\n",
    "model.save_pretrained(\"./lora_finetuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"./lora_finetuned_gpt2\")\n",
    "\n",
    "# 13. æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹\n",
    "prompt = \"To be or not to be,\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33589b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
